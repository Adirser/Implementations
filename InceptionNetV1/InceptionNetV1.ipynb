{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nati\\Desktop\\Implementations\\ImplementationsVenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import ppscore as pps\n",
    "import ppscore as pps\n",
    "import random\n",
    "from torchsummary import summary\n",
    "pd.set_option(\"display.max_columns\",80)\n",
    "pd.set_option(\"display.max_rows\",80)\n",
    "import datetime as dt\n",
    "from tslumen import HtmlReport\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import tqdm as notebook_tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.utils.data import Dataset , DataLoader \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](inception_block.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "       return self.relu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Inception_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
    "        super(Inception_block, self).__init__()\n",
    "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size=(1, 1))\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "        conv_block(in_channels, red_3x3, kernel_size=(1, 1)),\n",
    "        conv_block(red_3x3, out_3x3, kernel_size=(3, 3), padding=(1, 1)),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "        conv_block(in_channels, red_5x5, kernel_size=(1, 1)),\n",
    "        conv_block(red_5x5, out_5x5, kernel_size=(5, 5), padding=(2, 2)),\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "        nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "        conv_block(in_channels, out_1x1pool, kernel_size=(1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, aux_logits=True, num_classes=1000):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        assert aux_logits == True or aux_logits == False\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        # Write in_channels, etc, all explicit in self.conv1, rest will write to\n",
    "        # make everything as compact as possible, kernel_size=3 instead of (3,3)\n",
    "        self.conv1 = conv_block(\n",
    "        in_channels=3,\n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        )\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = conv_block(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # In this order: in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
    "        self.inception3a = Inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(3, 3), stride=2, padding=1)\n",
    "\n",
    "        self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc1 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        if self.aux_logits:\n",
    "            self.aux1 = InceptionAux(512, num_classes)\n",
    "            self.aux2 = InceptionAux(528, num_classes)\n",
    "        else:\n",
    "            self.aux1 = self.aux2 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.conv3(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.inception4a(x)\n",
    "\n",
    "        # Auxiliary Softmax classifier 1\n",
    "        if self.aux_logits and self.training:\n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "\n",
    "        # Auxiliary Softmax classifier 2\n",
    "        if self.aux_logits and self.training:\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        if self.aux_logits and self.training:\n",
    "            return aux1, aux2, x\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1000])\n"
     ]
    }
   ],
   "source": [
    "# N = 3 (Mini batch size)\n",
    "x = torch.randn(3, 3, 224, 224)\n",
    "model = GoogLeNet(aux_logits=True, num_classes=1000)\n",
    "print(model(x)[2].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImplementationsVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbcf07c5281d25e0992701b85e38716a4c7d6f793bca9ee5ebc4122174861675"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
