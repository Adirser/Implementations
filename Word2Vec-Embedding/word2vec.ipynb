{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to install BeautifulSoup for webscraping and lxml to parse HTML and xml files\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Wiki page data\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "# Read\n",
    "article = scrapped_data.read()\n",
    "# Parse article using bs\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\t\\t\\tPages '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt',download_dir=r\"C:\\Users\\nati\\Desktop\\Implementations\\Word2Vec-Embedding\")\n",
    "# Cleaing the text\n",
    "processed_article = article_text.lower()\n",
    "#Removing non-alphabetic characters such as numbers , punctuations and special characters\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "# Replacing one or MORE whitespaces with a single space\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "# Preparing the dataset - tokenization(list of sequences)\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "#Works tokenization\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NLTK_DATA'] = r\"C:\\Users\\nati\\Desktop\\Implementations\\Word2Vec-Embedding\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(r\"C:\\Users\\nati\\Desktop\\Implementations\\Word2Vec-Embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords',download_dir=r\"C:\\Users\\nati\\Desktop\\Implementations\\Word2Vec-Embedding\")  # Download stopwords data\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gensim for word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.key_to_index.keys()\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('intelligence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('games', 0.4014623463153839),\n",
       " ('ai', 0.363175630569458),\n",
       " ('machines', 0.35333162546157837),\n",
       " ('behavior', 0.33901742100715637),\n",
       " ('approaches', 0.3239952325820923),\n",
       " ('agent', 0.3110249638557434),\n",
       " ('popular', 0.30916503071784973),\n",
       " ('mind', 0.30537280440330505),\n",
       " ('may', 0.30257585644721985),\n",
       " ('neural', 0.30147451162338257)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImplementationsVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbcf07c5281d25e0992701b85e38716a4c7d6f793bca9ee5ebc4122174861675"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
